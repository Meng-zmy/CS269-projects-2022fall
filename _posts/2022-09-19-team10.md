---
layout: post
comments: true
title: Human-AI Shared Control via Policy Dissection
author: Ruikang Wu and Mengyuan Zhang (Team 10)
date: 2022-10-19
---

> In many complex tasks, RL-trained policy may not solve the tasks efficiently. The training process may cost people too much time. Policy dissection is a frequency-based method, which can convert RL-trained policy into target-conditioned policy. For this method, we can interacte with the kinematic behavior and the learned neural controller. In this project, we want to explore the implementation of the policy dissection. We will figure out how we can use the policy deissection to solve some potential problems in the real world.


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
This article will refer to the [source code](https://github.com/metadriverse/policydissect) .

First of all, we want to explore the relationship between kinematic behavior and the neural activities. In this experiment, we want to figure out the patterns behind some certain behaviors. 

Second, we want to train bipedal robots, Cassie, in [IsaacGym](https://github.com/NVIDIA-Omniverse/IsaacGymEnvs), based on the patterns that we figured out. The robots will be trained to have some complex behaviors such as crouch, turn left, turn right, back flip, tiptoe, jump etc. 

Third, we want to explore whether the bipedal robots can handle more complex situtions within the human instruction.

Fourth, we want to explore whether we can improve the algoirthm. For improving the algorithm, we may potentially compare the policy based, value-based and model based methods, and choose the best performance methods.

Finally, we will try to figure out whther we can implement the policy dissection to the other complex tasks in the real world. We are still working on finding the real world problems. 

## Human-AI Shared Control
The present human-AI shared chontrol methods can be roughly divided into two categories. The first category is involving training with a human control and testing the trained policy withou the human control [5]. The other is to have human carry out human auxiliary tasks throughout both training and testing. In policy dissestion method, it allows the human cooperate with AI during the testing, but human does not participate in training.

In the test, humans need to trigger some subtle operations when necessary, such as shifting to the left and jumping, to help better complete the task.

## Policy Dissection
- What is the Policy Dissection?

    Previous reinforcement learning methods have attempted to design goals-conditioned policies that can achieve human goals, but at the cost of redesigning reward functions and training paradigms. Hence the Policy Dissection, an approach inspired by neuroscience methods based on the primate motor cortex. Using this approach, manually controllable policies can be derived in trained reinforcement learning agents. Experiments show that under the blessing of Policy Dissection, the motion robot exhibits a variety of controllable motor skills.

- Four main steps for Policy Dissection[1]

    **1. Monitoring Neural Activity and Kinematics:** The policy dissection expands the trained policy and records the tracked neural activities and kinematic attributes, such as velocity and yaw. It will recorde the neural activities and kinematic attributes for further anaylsis.

    **2. Associating Units with Kinematic Attributes:** According to the records of neural activities and kinematic attributes, the same kinematic patterns will appear with different frequencies. So for a kinematic patterns, the unit with the smallest frequency discrepancy is the motor primitive. 

    **3. Building Stimulation-evoked Map:** Behavior can be described by changing a subset of kinematic attributes, or by activating a corresponding set of motor primitives. These movements are associated with certain behaviors to generate building blocks that include stimulation-evoked map.

    **4. Steering Agent via Stimulation-evoked Map:** Activate the kinematic attribute associated with the motion property and apply its derivative to the agent. The goal is to find features that can be easily scaled up or down kinematic attributes by selecting a unit.

- Workflow of Policy Dissection

    ![workflow]({{ '/assets/images/team10/algorithm.png'| relative_url}})
    {: style="width: 400px; max-width: 100%;"}
    *Fig 1. workflow of Policy Dissection* [1].

## PPO Result on MetaDrive
![video of policy dissection]({{ '/assets/images/team10/metadrive_without.mp4'| relative_url}}){: style="width: 400px; max-width: 100%;"}

## Policy Dissection Result
Here we provide an example of Human-AI shared control through policy dissection method. Through human control, we let the car successfully avoid obstacles and reach the end point.

![video of policy dissection]({{ '/assets/images/team10/metadrive_dissection.mp4'| relative_url}}){: style="width: 400px; max-width: 100%;"}

### PPO with Policy Dissection vs PPO
| Algorithm | Episode Reward | Episode Cost | Arriving Destination |
| --- | ----------- | ---------- | ------- |
| PPO with Policy Dissection | 438.050 | 0.0 | True |
| PPO | 345.505 | 17.0 | True |

## Installation
### Basic Installation
We completed the basic installation according to the [
policy dissect's](https://github.com/metadriverse/policydissect.git) instructions. It contains some basic packages we need.

### Environment Installation
We also installed supported environments for testing the policy dissection method.
- [MetaDrive](https://github.com/metadriverse/metadrive.git)

    ![metadrive]({{ '/assets/images/team10/metadrive.png'| relative_url}}){: style="width: 400px; max-width: 100%;"}

## Reference
[1] Q. Li, Z. Peng, H. Wu, L. Feng, and B. Zhou. Human-AI shared control via policy dissection. *arXiv preprint arXiv:2206.00152*, 2022. 

[2] D. Bau, J.-Y. Zhu, H. Strobelt, A. Lapedriza, B. Zhou, and A. Torralba. Understanding the role of individual units in a deep neural network. *Proceedings of the National Academy of Sciences*, 117(48):30071–30078, 2020.

[3] S. Guo, R. Zhang, B. Liu, Y. Zhu, D. Ballard, M. Hayhoe, and P. Stone. Machine versus human attention in deep reinforcement learning tasks. *Advances in Neural Information Processing Systems*, 34, 2021.

[4] B. Zhou, D. Bau, A. Oliva, and A. Torralba. Interpreting deep visual representations via network dissection. *IEEE transactions on pattern analysis and machine intelligence*, 41(9):2131–2145, 2018.

[5] R. Zhang, F. Torabi, G. Warnell, and P. Stone. Recent advances in leveraging human guidance for sequential decision-making tasks. *Autonomous Agents and Multi-Agent Systems*, 35(2):1–39, 2021.

[6] Q. Li, Z. Peng, Z. Xue, Q. Zhang, and B. Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. *arXiv preprint arXiv:2109.12674*, 2021.